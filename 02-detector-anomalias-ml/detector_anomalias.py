import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# ============================================
# 1. GERAR DADOS DE EXEMPLO (Logs simulados)
# ============================================

np.random.seed(42)

# Comportamento NORMAL
logs_normais = {
    'timestamp': pd.date_range('2024-01-01', periods=800, freq='1min'),
    'tentativas_login': np.random.normal(5, 2, 800),
    'requisicoes_http': np.random.normal(100, 20, 800),
    'bytes_transferidos': np.random.normal(50000, 10000, 800),
    'tempo_resposta_ms': np.random.normal(200, 50, 800),
}

df_normal = pd.DataFrame(logs_normais)
df_normal['tipo'] = 'Normal'

# Comportamento AN√îMALO (ataques simulados)
logs_anomalos = {
    'timestamp': pd.date_range('2024-01-01 13:20', periods=50, freq='1min'),
    'tentativas_login': np.random.normal(50, 10, 50),
    'requisicoes_http': np.random.normal(500, 100, 50),
    'bytes_transferidos': np.random.normal(500000, 100000, 50),
    'tempo_resposta_ms': np.random.normal(5000, 1000, 50),
}

df_anomalo = pd.DataFrame(logs_anomalos)
df_anomalo['tipo'] = 'An√¥malo'

# Combinar dados
df = pd.concat([df_normal, df_anomalo], ignore_index=True)
df = df.sort_values('timestamp').reset_index(drop=True)

# Salvar dados
df.to_csv('dados/logs_exemplo.csv', index=False)
print(f"‚úÖ Dados gerados: {len(df)} registros")
print(f"   - Normais: {len(df_normal)}")
print(f"   - An√¥malos: {len(df_anomalo)}\n")

# ============================================
# 2. PREPARAR DADOS PARA O MODELO
# ============================================

features = ['tentativas_login', 'requisicoes_http', 'bytes_transferidos', 'tempo_resposta_ms']
X = df[features].copy()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ============================================
# 3. TREINAR O MODELO (Isolation Forest)
# ============================================

modelo = IsolationForest(
    contamination=0.05,
    random_state=42,
    n_estimators=100
)

df['anomalia_pred'] = modelo.fit_predict(X_scaled)
df['anomalia_score'] = modelo.score_samples(X_scaled)

df['eh_anomalia'] = df['anomalia_pred'].apply(lambda x: 'Sim' if x == -1 else 'N√£o')

# ============================================
# 4. AN√ÅLISE DOS RESULTADOS
# ============================================

print("=" * 60)
print("üìä RESULTADOS DA DETEC√á√ÉO DE ANOMALIAS")
print("=" * 60)

anomalias_detectadas = (df['anomalia_pred'] == -1).sum()
acuracia = (df['eh_anomalia'] == df['tipo'].apply(lambda x: 'Sim' if x == 'An√¥malo' else 'N√£o')).sum() / len(df) * 100

print(f"\nüîç Anomalias Detectadas: {anomalias_detectadas}")
print(f"‚úÖ Acur√°cia do Modelo: {acuracia:.2f}%")
print(f"\nüìà Estat√≠sticas das Features:")
print(df[features].describe())

print(f"\nüö® Registros An√¥malos Detectados:")
anomalias = df[df['anomalia_pred'] == -1][['timestamp', 'tentativas_login', 'requisicoes_http', 'bytes_transferidos', 'tempo_resposta_ms']]
print(anomalias.head(10))

# ============================================
# 5. SALVAR RESULTADOS
# ============================================

df.to_csv('dados/logs_com_anomalias.csv', index=False)
print(f"\nüíæ Resultados salvos em 'logs_com_anomalias.csv'")

# ============================================
# 6. VISUALIZA√á√ïES
# ============================================

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Detec√ß√£o de Anomalias em Logs - Machine Learning', fontsize=16, fontweight='bold')

axes[0, 0].scatter(df.index, df['tentativas_login'], c=df['anomalia_pred'], cmap='RdYlGn_r', alpha=0.6)
axes[0, 0].set_title('Tentativas de Login ao Longo do Tempo')
axes[0, 0].set_xlabel('√çndice de Registro')
axes[0, 0].set_ylabel('Tentativas')
axes[0, 0].grid(True, alpha=0.3)

axes[0, 1].scatter(df.index, df['requisicoes_http'], c=df['anomalia_pred'], cmap='RdYlGn_r', alpha=0.6)
axes[0, 1].set_title('Requisi√ß√µes HTTP ao Longo do Tempo')
axes[0, 1].set_xlabel('√çndice de Registro')
axes[0, 1].set_ylabel('Requisi√ß√µes')
axes[0, 1].grid(True, alpha=0.3)

axes[1, 0].scatter(df.index, df['bytes_transferidos'], c=df['anomalia_pred'], cmap='RdYlGn_r', alpha=0.6)
axes[1, 0].set_title('Bytes Transferidos ao Longo do Tempo')
axes[1, 0].set_xlabel('√çndice de Registro')
axes[1, 0].set_ylabel('Bytes')
axes[1, 0].grid(True, alpha=0.3)

axes[1, 1].scatter(df.index, df['tempo_resposta_ms'], c=df['anomalia_pred'], cmap='RdYlGn_r', alpha=0.6)
axes[1, 1].set_title('Tempo de Resposta ao Longo do Tempo')
axes[1, 1].set_xlabel('√çndice de Registro')
axes[1, 1].set_ylabel('Tempo (ms)')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('visualizacoes_anomalias.png', dpi=300, bbox_inches='tight')
print(f"üìä Gr√°ficos salvos em 'visualizacoes_anomalias.png'")
plt.show()

# ============================================
# 7. GR√ÅFICO DE DISTRIBUI√á√ÉO DE ANOMALIAS
# ============================================

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

contagem = df['eh_anomalia'].value_counts()
axes[0].bar(contagem.index, contagem.values, color=['#2ecc71', '#e74c3c'])
axes[0].set_title('Distribui√ß√£o: Normal vs An√¥malo', fontweight='bold')
axes[0].set_ylabel('Quantidade')
axes[0].grid(True, alpha=0.3, axis='y')

axes[1].hist(df['anomalia_score'], bins=50, color='#3498db', edgecolor='black', alpha=0.7)
axes[1].axvline(df[df['anomalia_pred'] == -1]['anomalia_score'].max(), color='red', linestyle='--', linewidth=2, label='Limiar de Anomalia')
axes[1].set_title('Distribui√ß√£o de Anomaly Scores', fontweight='bold')
axes[1].set_xlabel('Anomaly Score')
axes[1].set_ylabel('Frequ√™ncia')
axes[1].legend()
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('distribuicao_anomalias.png', dpi=300, bbox_inches='tight')
print(f"üìä Distribui√ß√£o salva em 'distribuicao_anomalias.png'")
plt.show()

# ============================================
# 8. GERAR RELAT√ìRIO EXECUTIVO
# ============================================
with open('resultados/relatorio_anomalias.txt', 'w', encoding='utf-8') as f:
    f.write("=" * 60 + "\n")
    f.write("RELAT√ìRIO DE DETEC√á√ÉO DE ANOMALIAS\n")
    f.write("=" * 60 + "\n\n")
    f.write(f"Total de registros analisados: {len(df)}\n")
    f.write(f"Anomalias detectadas: {anomalias_detectadas}\n")
    f.write(f"Taxa de anomalias: {(anomalias_detectadas/len(df))*100:.2f}%\n")
    f.write(f"Acur√°cia do modelo: {acuracia:.2f}%\n\n")
    f.write("Top 10 Anomalias Detectadas:\n")
    f.write(anomalias.head(10).to_string())

print("‚úì Relat√≥rio salvo: resultados/relatorio_anomalias.txt")

# ============================================
# 9. AN√ÅLISE AVAN√áADA COM IA
# ============================================
print("\n[9] Gerando an√°lise avan√ßada com IA (Mistral via Ollama)...")

from ia_anomalias import analisar_anomalias_com_llm

with open('resultados/relatorio_anomalias.txt', 'r', encoding='utf-8') as f:
    texto_base = f.read()

analise_ia = analisar_anomalias_com_llm(texto_base)

with open('resultados/relatorio_ia_avancado.txt', 'w', encoding='utf-8') as f:
    f.write(analise_ia)

print("‚úì An√°lise IA salva: resultados/relatorio_ia_avancado.txt")


print("\n" + "=" * 60)
print("‚úÖ PROJETO CONCLU√çDO COM SUCESSO!")
print("=" * 60)
